\documentclass[12pt,a4paper]{article} 

\usepackage{titlesec}
\usepackage{color}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\graphicspath{{Images/}}
\usepackage{eso-pic} 
\usepackage{subfig} 
\usepackage{caption} 
\usepackage{transparent}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{bm}
\usepackage[overload]{empheq}  
\usepackage{tabularx}
\usepackage{longtable}
\usepackage{colortbl}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[colorlinks=true,linkcolor=black,anchorcolor=black,citecolor=black,filecolor=black,menucolor=black,runcolor=black,urlcolor=black]{hyperref} 
\usepackage{cleveref}
\usepackage[square, numbers]{natbib} 
\bibliographystyle{unsrt}
\usepackage{appendix}
\usepackage{enumitem}
\usepackage{amsthm,thmtools,xcolor} 
\usepackage{comment} 
\usepackage{fancyhdr} 
\usepackage{lipsum} 
\usepackage{tcolorbox} 
\newcommand{\bea}{\begin{eqnarray}} 
\newcommand{\eea}{\end{eqnarray}}
\newcommand{\e}[1]{\times 10^{#1}}  
\newcommand{\mathbbm}[1]{\text{\usefont{U}{bbm}{m}{n}#1}} 
\newcommand{\pdev}[2]{\frac{\partial#1}{\partial#2}}

\input{Configuration_files/config}

% -> title of your work
\renewcommand{\title}{KANs: Kolmogorov-Arnold Networks}
% -> author name and surname
\renewcommand{\author}{Pasqual Matteo Romilio}
% -> MSc course
\newcommand{\course}{Master's degree in Computer Science Engineering }
% -> advisor name and surname
\newcommand{\advisor}{Prof. Edie Miglio}
% -> author ID
\newcommand{\ID}{10765765}
% -> academic year
\newcommand{\YEAR}{2024-2025}
% -> abstract (only in English)
\renewcommand{\abstract}{Here goes Abstract }

% -> key-words (only in English)
\newcommand{\keywords}{}

%------------------
%	BEGIN OF YOUR DOCUMENT
%------------------
\begin{document}
\input{Configuration_files/title_page}

%--------------
% PAPER
%--------------
\section{Introduction}
\label{sec:in}
Multi-layer perceptrons (MLPs), also known as fully-connected feedforward neural networks, are foundational building blocks of today’s deep learning models. The importance of MLPs can never be overstated, since they are the default models in machine learning for approximating non-linear functions, due to their expressive power guaranteed by the universal approximation theorem. 

However, are MLPs the best non-linear regressors we can build? Despite the prevalent use of MLPs, they have significant drawbacks. In transformers, for example, MLPs consume almost all non-embedding parameters and are typically less interpretable (relative to attention layers) without post-analysis tools.

We propose a promising alternative to MLPs, called Kolmogorov-Arnold Networks (KANs). 

Whereas MLPs are inspired by the universal approximation theorem, KANs are inspired by the Kolmogorov-Arnold representation theorem. Like MLPs, KANs have fully-connected structures. However, while MLPs place fixed activation functions on nodes “neurons”), KANs place learnable activation functions on edges (“weights”), as illustrated in Figure~\ref{fig:MLPKAN}. As a result, KANs have no linear weight matrices at all: instead, each weight parameter is replaced by a learnable 1D function parametrized as a spline. KANs’ nodes simply sum incoming signals without applying any non-linearities. 

One might worry that KANs are hopelessly expensive since each MLP’s weight parameter becomes KAN’s spline function. Fortunately, KANs usually allow much smaller computation graphs than MLPs embedding a 2-hidden-layer neural network.

Throughout this paper, we will show that KANs can lead to higher accuracy and improved interpretability over MLPs. We will show how, by leveraging the Kolmogorov-Arnold representation theorem, learnable activation functions and the absence of fixed-weight matrices allow KANs to efficiently capture complex relationships within the data\cite{KAN}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{Images/MLP-KAN.png}
    \caption{Multi-layer perceptrons (MLPs) vs Kolmogorov-Arnold Networks (KANs)}
    \label{fig:MLPKAN}
\end{figure}

\section{Kolmogorov-Arnold representation theorem}
\label{sec:ka}
Whereas Multi-layer perceptrons (MLPs) are inspired by the universal approximation theorem, Kolmogorov-Arnold Networks (KANs) are inspired by the Kolmogorov-Arnold representation theorem \cite{KAN}.

Firstly, we start with the basic concept that a neural network can be described as a multivariate continuous function $f: \Re^n \rightarrow \Re^m $ that maps an input vector $\textbf{x}\in \Re^n$ to an output vector $\textbf{y}\in \Re^m$ via a series of computations. In particular, both MLPs and KANs can be described as a multivariate continuous function $f: \Re^n \rightarrow \Re $ that maps an input vector $\textbf{x}\in \Re^n$ to an output value $y\in \Re$ via a series of computations: $f(x) \equiv KAN(x)$ \cite{book1NAML, book2NAML}.

As in MLP when input variables are combined linearly we have to standardize inputs passing from a domain $D \subset \Re^n$ to a compact domain $D \subset [0,1]^n $. A possibility is to apply an affine transformation to the data so that each feature will be normalized in $[0,1] \in 
 \Re$.

\subsection{Theorem}
We can now formally present and demonstrate the Kolmogorov-Arnold representation theorem. 

The Kolmogorov-Arnold representation theorem is essential for proving that any neural network that has one output (for instance MLPs) can always be rebuilt as two-hidden-layer KANs. In particular, the Kolmogorov-Arnold representation theorem states that if $f$ is a multivariate continuous function on a bounded domain, then it can be written as a finite composition of continuous functions of a single variable and the binary operation of addition \cite{KAtheorem, KArevisited}.

\begin{theorem}[Kolmogorov-Arnold representation theorem \cite{KAtheorem}] 
\label{Kolmogorov-Arnold}
Let $f$ be an arbitrary multivariate continuous function on a bounded domain $f:[0,1]^n -> \Re$, then $f$:

$$f(\textbf{x}) = f(x_1,x_2,...,x_n) = \sum_{q=1}^{2n+1} \Phi_q(\sum_{p=1}^n \phi_{q,p}(x_p))$$

with continuous one–dimensional inner functions $\phi_{q,p}:[0,1] -> \Re$ and continuous one–dimensional outer functions $\Phi_q:\Re -> \Re$.
\end{theorem} 

In a sense, they showed that the only true multivariate function is addition since every other function can be written using univariate functions and sum. The great advantage is that we have to learn only a polynomial number of 1D functions. However, these 1D functions can be non-smooth and even fractal, so they may not be learnable; in practice with spline function, we can approximate their shapes and achieve the accuracy that we want \cite{KAN}.

\subsection{Matrix form}
\label{sec:ma}
For a more formal definition, we define the representation matrix of the Kolmogorov-Arnold representation theorem ( $f: [0,1]^n \rightarrow \Re$ ) \cite{KAN}:

$$f(\textbf{x}) = \boldsymbol{\Phi_{out}} \times \boldsymbol{\Phi_{in} }\times \textbf{x}$$

where $\textbf{x} \in \Re^n$, $\boldsymbol{\Phi_{in}} \in \Re^{2n+1,n}$, and $\boldsymbol{\Phi_{out}} \in \Re^{2n+1}$:

\[
\boldsymbol{\Phi_{in}} = 
\begin{pmatrix}
\phi_{1,1}(\cdot) & \dots & \phi_{1,n}(\cdot) \\
\vdots &   & \vdots \\
\phi_{2n+1,1}(\cdot) & \dots & \phi_{2n+1,n}(\cdot)
\end{pmatrix},
\quad \boldsymbol{\Phi_{in}} = (\Phi_{1}(\cdot) \dots \Phi_{2n+1}(\cdot))
\]

We notice that both $\boldsymbol{\Phi_{in}}$ and $\boldsymbol{\Phi_{out}}$ are special cases of the following function matrix $\boldsymbol{\Phi} \in \Re^{n_{out},n_{in}}$ that represent a general Kolmogorov-Arnold layer. 

\[
\boldsymbol{\Phi} = 
\begin{pmatrix}
\phi_{1,1}(\cdot) & \dots & \phi_{1,n_{in}}(\cdot) \\
\vdots &   & \vdots \\
\phi_{n_{out},1}(\cdot) & \dots & \phi_{n_{out},n_{in}}(\cdot)
\end{pmatrix} 
\]

so each layer map the input of layer $l$ $\textbf{x}_l$ to the output $\textbf{x}_{l+1}$ thought $\boldsymbol{\Phi}$:

\[
\textbf{x}_{l+1} = \boldsymbol{\Phi} \times \textbf{x}_l \Rightarrow
\textbf{x}_{l+1} = 
\begin{pmatrix}
\phi_{1,1}(\cdot) & \dots & \phi_{1,n_{in}}(\cdot) \\
\vdots &   & \vdots \\
\phi_{n_{out},1}(\cdot) & \dots & \phi_{n_{out},n_{in}}(\cdot)
\end{pmatrix} \textbf{x}_l
\]


Each layer has $n_{in}$ inputs and $n_{out}$ outputs and is mapped thought its respective layer matrix. In our case, the two layers are defined as follows:
\begin{itemize}
    \item Layer 1: $\boldsymbol{\Phi_{in}}$ has $\boldsymbol{\Phi}$ with $n_{in} = n$ and $n_{out} = 2n+1$
    \item Layer 2: $\boldsymbol{\Phi_{out}}$ has $\boldsymbol{\Phi}$ with $n_{in} = 2n+1$ and $n_{out} = 1$
\end{itemize}

\section{KAN Structure}
Kolmogorov-Arnold networks require at least two layers, with a correspective depth of \(2n+1\) and \(n\). However, in practice, we can design larger networks with various structures beyond this minimum requirement \cite{KAN}.

We can construct various topologies of Kolmogorov-Arnold networks by simply stacking layers and following these rules:
\begin{itemize}
    \item A KAN, as all MPLs, is a directed acyclic graph (DAG). 
    \item Each KAN layer is fully connected to the preceding and succeeding layers apart from the first layer will only have a succeeding layer and the last layer will only have a preceding layer.
    \item Each KAN layer is not connected with other layers
    \item Each KAN layers connection $\textbf{x}_{l+1} = \boldsymbol{\Phi}_L \times \textbf{x}_l$ is defined by $\boldsymbol{\Phi_{L}} \in \Re^{n_{out},n_{in}}$ where $n_{in}$ is the dimension of the $\textbf{x}_l$ layer and $n_{out}$ is the dimension of the $\textbf{x}_l$ layer (Section~\ref{sec:ma}).
    \item Every edge of al layer $\phi_{l,q,p}$ is associated with a one-dimensional activation function. 
\end{itemize}

Let’s say we have a KAN with $L$ layers, where the $l^{th}$ layer has shape $n_{l+1}$, $n_l$ and has formula $\textbf{x}_{l+1} = \boldsymbol{\Phi} \times \textbf{x}_l $. Then the whole network will be represented by:

$$KAN(\textbf{x}) = \boldsymbol{\Phi_{L-1}} \times \dots \times \boldsymbol{\Phi_{1}} \times \boldsymbol{\Phi_{0}} \times \textbf{x}$$

We can also rewrite the above equation as a series of summations, assuming the output dimension $n_L = 1$ and the associated multivariate
continuous function $f$:  $f(x) \equiv KAN(x)$:

$$f(\textbf{x}) = \sum_{i_{L-1}=1}^{n_{L-1}}  \phi_{L-1,i_L,i_{L-1}} (...(\sum_{i_1=1}^{n_{1}}  \phi_{1,i_2,i_{1}}(\sum_{i_0=1}^{n_{0}}  \phi_{0,i_1,i_{0}}(x_{i_0}))...)$$

The network structure will be only a series of matrix-matrix multiplication of $ \boldsymbol{\Phi_{L-1}}, \dots, \boldsymbol{\Phi_{1}},\boldsymbol{\Phi_{0}}$  while a MLPs was a series of linear transformations $W_l$ and non-linear transformations $\sigma$:

$$MLP(\textbf{x}) = \boldsymbol{W_{L-1}} \times \sigma \times \dots \times \boldsymbol{W_{1}} \times \sigma \times\boldsymbol{W_{0}} \times \textbf{x}$$

Finally, a KAN can be conceptualized as a structured stack of KAN layers. Each KAN layer can be represented as a fully connected layer where every edge is associated with a one-dimensional (1D) function. The critical components that need to be defined are the representation and training of these activation functions which are the only learnable part of the function.

\subsection{Activation functions}
While the equation to compute, given an input, the KAN's output appears very simple, ensuring that the activation functions are well-trained is more complex. The most efficient technique involves leveraging spline functions. These functions are particularly effective for approximating the complexity and the non-linearities of the 1D functions associated with the KAN layers. This approach enhances both the flexibility and trainability of the network \cite{KAN}.

Formally speaking, in KANs, every activation function $\phi_{l,q,p}(\cdot)$ is defined as follows:
$$\phi(x) = w_bb(x) + w_sspline(x) $$

where:
\begin{itemize}
    \item $w_b$ and $w_s$ are learnable weights; in principle are redundant since they can be absorbed into $b(x)$ and $spline(x)$, we still include these factors to better control the overall magnitude.
    \item $b(x)$ is a fixed function called residual connection function defined by the $silu(x)$ function $b(x) = silu(x) = \frac{x}{1+e^{-x}}$
    \item $spline(x)$ that is the real learnable function and in most of the cases is parametrized as a linear combination of B-splines function $spline(x)= \sum_i c_iB_i(x)$ where $c_i$s a re learnable and $i$ is the spline order.
\end{itemize}

\subsection{Bézier and B-splines functions}
Given the spline function now we have to define its shape. In literature, two main classes of functions are designed for KANs:
\begin{itemize}
    \item Bézier functions which consider all the domains but are complex to train \cite{bezier}
    \item B-splines functions which consider only a local domain but are easy to train \cite{kan_intro}.
\end{itemize}

The problem we are going to solve is that the spline function should pass through some tag points that will be adjusted during the training phase. To solve it let's consider a dual problem: imagine a character $C$ must pass through $n$ points $(P_1, \dots,P_n)$. The most obvious way to traverse them is to go straight from $P_i$ to $P_{i+1}$ but this movement does not appear natural because we desire a smooth traversal movement that can be described as $(n-1)$-degree polynomial as in Figure~\ref{fig:bezier}.  
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{Images/bezier.png}
    \caption{Example of 4-degree polynomial curve}
    \label{fig:bezier}
\end{figure}

The naive way to implement it is to define a function $h(x): \Re \to \Re$:

$$h(x) = a_{n-1}x^{n-1} + a_{n-2}x^{n-2} + \dots + a_1x +a_0 $$

then we can substitute points $(P_1, \dots,P_n)$ into the function $h$ and determine the values of the coefficients $(a_0, \dots,a_{n-1})$. Now we realize that we have to solve $N$ equations to determine the coefficients. Solving such a system of linear equations will be computationally expensive and almost infeasible for KANs. For this reason, we choose Bézier or B-splines functions.

\subsubsection{Bézier}
Bézier curves solve the problem more smartly. They provide a way to represent a smooth curve that passes near a set of control points without needing to solve a large system of equations as in Figure~\ref{fig:bezier2}.   
\begin{figure}[H]
    \centering
    \subfloat{%
        \includegraphics[width=0.55\linewidth]{Images/bezier2.png}%
        \label{fig:bezier2a}%
    }
    \hfill
    \subfloat{%
        \includegraphics[width=0.30\linewidth]{Images/bezier3.png}%
        \label{fig:bezier2b}%
    }
    \caption{Example of 4-degree bezier curves}
    \label{fig:bezier2}
\end{figure}


We define the Bézier curve $b(t): \Re \to \Re$ noticing from low $n$ polynomials as in Figure~\ref{fig:bezier2} that the coefficients match with the binomial coefficients of $(1+t)^n$. Then we can derive the binomial definition of the Bézier curve as:

\[
\mathbf{b}(t) = \sum_{i=0}^n \binom{n}{i} (1-t)^{n-i} t^i \mathbf{P}_i, \quad t \in [0, 1]
\]

However, the problem is still the same as before. Having $N$ data points will result in a polynomial of degree $N-1$, which will be computationally expensive.

\subsubsection{B-splines}
B-splines provide a more efficient way to represent curves, especially when we deal with a large number of data. Unlike high-degree polynomials, B-splines use a series of lower-degree polynomial segments, which are connected smoothly.
In other words, instead of extending Bézier curves to tens of hundreds of data points, which leads to an equally high degree of the polynomial, we use multiple lower-degree polynomials and connect them to form a smooth curve as in Figure~\ref{fig:bezier4}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{Images/bezier4.png}
    \caption{Example of 6-degree B-spline curve}
    \label{fig:bezier4}
\end{figure}

When we have $n$ control points and we create $k$ degree polynomial Bézier curves, we get $(n-k)$ Bézier curves in the final Bsplines. We ensure also a certain continuity condition at the points where the curves meet:
\begin{itemize}
    \item Position Continuity: $C^0$ Continuity 
    \item Tangent Continuity: $C^1$ Continuity  
    \item Curvature Continuity: $C^2$ Continuity 
\end{itemize}

Similar to Bézier curves, we define the B-spline curve \( B(t): \mathbb{R} \to \mathbb{R} \) as a linear combination of the points \( P_i \) and their associated basis functions:


\[
\mathbf{B}(t) = \sum_{i=0}^n \mathbf{P}_i N_{i,k},
\]

Finally, B-spline functions are the best choice for KANs, even though they consider only a local domain, because their strength lies in their easy trainability, even as the number of points increases.


\subsection{Initialization and training}

\subsection{Hyperparameters and complexity}
The hyperparameters of a Kolmogorov-Arnold network are:
\begin{itemize}
    \item \textbf{L}: the depth of the KAN
    \item \textbf{N} = $\textbf{n}_0$, ... ,$\textbf{n}_l$: width of each layer
    \item \textbf{G},\textbf{k}: each spline is of order k (usually k = 3) on G intervals.
\end{itemize}

Then a KAN with hyperparameters \textbf{L},\textbf{N},\textbf{G},\textbf{k} will have in total $O(N^2L(G + k))  \simeq O(N^2LG)$ parameters. In contrast an MLP with hyperparameters \textbf{L, N} needs only  $O(N^2L)$ parameters, which appears to be more efficient than KAN. Fortunately, KANs usually require much smaller N than MLPs, which not only saves parameters but also achieves better generalization and facilitates interpretability \cite{KAN}. 


\section{Conclusions}

%--------------
% END
%--------------
\bibliography{bibliography.bib}
\end{document}